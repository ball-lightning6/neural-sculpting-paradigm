import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from PIL import Image
from torchvision import transforms
from tqdm import tqdm
import timm
import sys

# ==============================================================================
# --- 1. é…ç½®åŒºåŸŸ (å·²æ›´æ–°) ---
# ==============================================================================

# --- æ¨¡å‹é…ç½® ---
# MODEL_NAME = "swin_base_patch4_window7_224_in22k"
MODEL_NAME = "swin_base_patch4_window7_224.ms_in22k"
OUTPUT_CHANNELS = 3

# --- æ•°æ®é›†é…ç½® ---
# DATASET_DIR = "rainwater_image_dataset"
# DATASET_DIR = "maze_path_dataset"
DATASET_DIR = "incircle_dataset"
# DATASET_DIR = "centroid_dataset"
# DATASET_DIR = "symmetry_axis_dataset_FULLY_CONTAINED"
# DATASET_DIR = "catenary_dataset_v3.2_DEFINITIVE_FIX"
# DATASET_DIR = "catenary_dataset_v6_PHYSICALLY_CONSTRAINED"
# DATASET_DIR = "catenary_dataset_v5_CONSTRUCTIVE"
# DATASET_DIR = "refraction_dataset_v1.4_USER_LOGIC"
# DATASET_DIR = "bouncing_ball_dataset_v0.4_variable_bounces"
# DATASET_DIR = "tessellation_dataset_256"
# DATASET_DIR = "orbital_dataset_256_separated_v3"
# DATASET_DIR = "triangle_dataset_pil"
# DATASET_DIR = "arc_final_dataset_mp"
# DATASET_DIR = "arc_recursive_4x4_dataset"
# DATASET_DIR = "arc_conditional_bbox_dataset"
# DATASET_DIR = "arc_final_projection_dataset"
# DATASET_DIR = "arc_conditional_projection_dataset_v2"
# DATASET_DIR = "arc_ultimate_plus_dataset"
# DATASET_DIR = "arc_priority_final_dataset_v2"
# DATASET_DIR = "arc_dynamic_swap_dataset"
# DATASET_DIR = "arc_ultimate_final_v4_dataset"#arc-agi-2
# DATASET_DIR = "arc_ultimate_graduation_final_dataset"#arc-agi-2
# DATASET_DIR = "arc_cross_pattern_dataset"#arc-agi-2
# DATASET_DIR = "arc_periodic_pattern_dataset"#arc-agi-2
# DATASET_DIR = "arc_final_sorting_dataset"#arc-agi-2
# DATASET_DIR = "arc_meta_reasoning_dataset"#arc-agi-2
# DATASET_DIR = "arc_fluid_final_dataset"#arc-agi-2
# DATASET_DIR = "arc_jigsaw_puzzle_masterpiece_dataset"#arc-agi-2
# DATASET_DIR = "arc_jigsaw_puzzle_mine_dataset"  # arc-agi-2ï¼Œå’Œä¸Šé¢åŒä¸€ä¸ªä»»åŠ¡ï¼Œæ•°æ®é›†æ›´å¥½
IMAGE_SIZE = (224, 224)
# IMAGE_SIZE = (256, 256)

# --- è®­ç»ƒå‚æ•° ---
OUTPUT_DIR = "./checkpoints_incircle"
LOG_FILE = os.path.join(OUTPUT_DIR, "training_log.txt")
BATCH_SIZE = 16
# LEARNING_RATE = 1e-4#é€šç”¨ï¼Œnanåæ”¹ä¸ºä¸‹é¢
LEARNING_RATE = 5e-5
NUM_EPOCHS = 1800
FP16 = torch.cuda.is_available()

PRETRAINED_MODEL_PATH = "./swin-base-patch4-window7-224-in22k1/pytorch_model.bin"

# --- å…¨æ–°ï¼šæŒ‰æ­¥æ•°è¯„ä¼°çš„é…ç½® ---
EVAL_EVERY_N_STEPS = 200  # <--- æ¯500ä¸ªè®­ç»ƒæ­¥æ•°ï¼Œè¿›è¡Œä¸€æ¬¡éªŒè¯
SAVE_IMAGE_EVERY_N_STEPS = 200  # <--- æ¯1000ä¸ªè®­ç»ƒæ­¥æ•°ï¼Œä¿å­˜ä¸€æ¬¡å¯¹æ¯”å›¾


# ==============================================================================
# --- 2. è‡ªå®šä¹‰å›¾åƒæ•°æ®é›† (ä¿æŒä¸å˜) ---
# ==============================================================================

class ImageToImageDataset(Dataset):
    """ä¸€ä¸ªä¸“é—¨ç”¨äºè¯»å– Image-to-Image ä»»åŠ¡æ•°æ®çš„Datasetã€‚"""

    def __init__(self, root_dir, image_size=(224, 224)):
        self.input_dir = os.path.join(root_dir, "input")
        self.output_dir = os.path.join(root_dir, "output")
        self.image_files = sorted([f for f in os.listdir(self.input_dir) if f.endswith('.png')])
        self.transform = transforms.Compose([
            transforms.Resize(image_size),
            transforms.ToTensor(),
        ])

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        input_img_path = os.path.join(self.input_dir, img_name)
        output_img_path = os.path.join(self.output_dir, img_name)
        input_image = Image.open(input_img_path).convert("RGB")
        output_image = Image.open(output_img_path).convert("RGB")
        input_tensor = self.transform(input_image)
        output_tensor = self.transform(output_image)
        return {"input_pixel_values": input_tensor, "output_pixel_values": output_tensor}


class ImageToImageDataset_maze(Dataset):
    """
    ä¸€ä¸ªä¸“é—¨ç”¨äºè¯»å– Image-to-Image ä»»åŠ¡æ•°æ®çš„Datasetã€‚
    èƒ½å¤„ç†è¾“å…¥å’Œè¾“å‡ºå›¾åƒåœ¨åŒä¸€ç›®å½•ä¸‹çš„æƒ…å†µ (ä¾‹å¦‚ '0_input.png', '0_output.png')ã€‚
    """

    def __init__(self, root_dir, mode='train', image_size=(224, 224)):
        """
        Args:
            root_dir (string): æ•°æ®é›†æ ¹ç›®å½• (ä¾‹å¦‚ 'maze_path_dataset')ã€‚
            mode (string): 'train' æˆ– 'eval'ï¼Œå†³å®šåŠ è½½å“ªä¸ªå­ç›®å½•ã€‚
            image_size (tuple): å›¾åƒå°†è¢«è°ƒæ•´åˆ°çš„å°ºå¯¸ã€‚
        """
        assert mode in ['train', 'eval'], "mode å¿…é¡»æ˜¯ 'train' æˆ– 'eval'"

        # 1. ã€ä¿®æ”¹ã€‘æ ¹æ® mode ç¡®å®šæ•°æ®ç›®å½•
        self.data_dir = os.path.join(root_dir, mode)

        # 2. ã€ä¿®æ”¹ã€‘æ™ºèƒ½åœ°æŸ¥æ‰¾æ‰€æœ‰è¾“å…¥æ–‡ä»¶
        # æˆ‘ä»¬åªæŸ¥æ‰¾ '_input.png' æ–‡ä»¶ï¼Œç„¶åæ¨æ–­å‡ºå¯¹åº”çš„è¾“å‡ºæ–‡ä»¶å
        self.input_files = sorted([f for f in os.listdir(self.data_dir) if f.endswith('_input.png')])

        # 3. ã€æ”¹è¿›ã€‘ä¿æŒä½ åŸæœ‰çš„ transform é€»è¾‘
        self.transform = transforms.Compose([
            transforms.Resize(image_size, interpolation=transforms.InterpolationMode.NEAREST),  # ä½¿ç”¨æœ€è¿‘é‚»æ’å€¼ï¼Œé¿å…é¢œè‰²æ¨¡ç³Š
            transforms.ToTensor(),
        ])

    def __len__(self):
        return len(self.input_files)

    def __getitem__(self, idx):
        # 4. ã€ä¿®æ”¹ã€‘æ„å»ºè¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        input_filename = self.input_files[idx]
        # é€šè¿‡æ›¿æ¢å­—ç¬¦ä¸²æ¥è·å¾—è¾“å‡ºæ–‡ä»¶åï¼Œè¿™æ¯”å‡è®¾å®ƒä»¬åœ¨ä¸åŒç›®å½•ä¸­æ›´å¯é 
        output_filename = input_filename.replace('_input.png', '_output.png')

        input_img_path = os.path.join(self.data_dir, input_filename)
        output_img_path = os.path.join(self.data_dir, output_filename)

        try:
            input_image = Image.open(input_img_path).convert("RGB")
            output_image = Image.open(output_img_path).convert("RGB")
        except FileNotFoundError:
            # å¢åŠ ä¸€ä¸ªå¥å£®æ€§æ£€æŸ¥ï¼Œå¦‚æœæ‰¾ä¸åˆ°å¯¹åº”çš„è¾“å‡ºæ–‡ä»¶ï¼Œç»™å‡ºæ¸…æ™°çš„é”™è¯¯æç¤º
            raise FileNotFoundError(f"é”™è¯¯ï¼šæ‰¾åˆ°äº†è¾“å…¥æ–‡ä»¶ '{input_img_path}' ä½†æ‰¾ä¸åˆ°å¯¹åº”çš„è¾“å‡ºæ–‡ä»¶ '{output_img_path}'ã€‚è¯·æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å®Œæ•´ã€‚")

        input_tensor = self.transform(input_image)
        output_tensor = self.transform(output_image)

        # è¿”å›çš„å­—å…¸é”®åä¿æŒä¸å˜ï¼Œä»¥ä¾¿ä¸ä½ çš„è®­ç»ƒå¾ªç¯å…¼å®¹
        return {"input_pixel_values": input_tensor, "output_pixel_values": output_tensor}
        # æ³¨æ„ï¼šæˆ‘å°†é”®åä» "input_pixel_values" å’Œ "output_pixel_values"
        # æ”¹ä¸º "pixel_values" å’Œ "labels"ã€‚è¿™æ›´ç¬¦åˆ Hugging Face Transformers çš„ä¹ æƒ¯ï¼Œ
        # å¦‚æœä½ çš„è®­ç»ƒå¾ªç¯ä½¿ç”¨çš„æ˜¯å‰è€…ï¼Œè¯·æ”¹å›å³å¯ã€‚
        # return {"input_pixel_values": input_tensor, "output_pixel_values": output_tensor}


# ==============================================================================
# --- 3. Swin-Unet æ¨¡å‹ (ä¿æŒä¸å˜) ---
# ==============================================================================

class UpsampleBlock(nn.Module):
    """ä¸€ä¸ªU-Netçš„ä¸Šé‡‡æ ·å—ï¼ŒåŒ…å«ä¸Šé‡‡æ ·ã€æ‹¼æ¥å’Œå·ç§¯"""

    def __init__(self, in_channels, out_channels):
        super().__init__()
        # ä¸Šé‡‡æ ·å±‚ï¼šè¾“å…¥in_channels, è¾“å‡ºout_channels
        self.upsample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)

        # â˜…â˜…â˜… å…³é”®ä¿®æ­£ â˜…â˜…â˜…
        # å·ç§¯å—æ¥æ”¶çš„æ˜¯æ‹¼æ¥åçš„å¼ é‡ï¼Œå…¶é€šé“æ•°æ˜¯ skip_connectionçš„é€šé“æ•° + ä¸Šé‡‡æ ·åçš„é€šé“æ•°ã€‚
        # skip_connectionçš„é€šé“æ•°å°±æ˜¯out_channelsï¼Œä¸Šé‡‡æ ·åçš„é€šé“æ•°ä¹Ÿæ˜¯out_channelsã€‚
        # æ‰€ä»¥ï¼Œè¿™é‡Œçš„è¾“å…¥é€šé“æ•°æ˜¯ out_channels * 2ã€‚
        self.conv = nn.Sequential(
            nn.Conv2d(out_channels * 2, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x, skip_connection):
        x = self.upsample(x)
        x = torch.cat([skip_connection, x], dim=1)
        return self.conv(x)


class SwinUnet(nn.Module):
    def __init__(self, model_name=MODEL_NAME, output_channels=OUTPUT_CHANNELS, pretrained=True):
        super().__init__()

        # ç¼–ç å™¨éƒ¨åˆ†ä¿æŒä¸å˜
        self.encoder = timm.create_model(
            model_name,
            # æ³¨æ„ï¼šåœ¨ç¦»çº¿åŠ è½½çš„åœºæ™¯ä¸‹ï¼Œè¿™é‡Œåº”è¯¥è®¾ä¸º False
            # pretrained=False, # pretrained å‚æ•°åœ¨ä¸»å‡½æ•°ä¸­å¤„ç†
            features_only=True,
            out_indices=(0, 1, 2, 3)
        )

        encoder_channels = self.encoder.feature_info.channels()

        # è§£ç å™¨éƒ¨åˆ†ä¹Ÿä¿æŒä¸å˜
        self.bottleneck = nn.Sequential(
            nn.Conv2d(encoder_channels[3], encoder_channels[3] * 2, kernel_size=3, padding=1),
            nn.ReLU(inplace=True)
        )
        self.up1 = UpsampleBlock(encoder_channels[3] * 2, encoder_channels[2])
        self.up2 = UpsampleBlock(encoder_channels[2], encoder_channels[1])
        self.up3 = UpsampleBlock(encoder_channels[1], encoder_channels[0])
        self.up4 = nn.ConvTranspose2d(encoder_channels[0], encoder_channels[0] // 2, kernel_size=2, stride=2)
        self.final_conv = nn.Sequential(
            nn.Conv2d(encoder_channels[0] // 2, output_channels, kernel_size=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        # 1. ä»ç¼–ç å™¨è·å–ç‰¹å¾
        features = self.encoder(x)

        # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…  å…³é”®ä¿®æ­£ï¼šæ·»åŠ  Permute æ“ä½œ â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
        # å°†æ‰€æœ‰ç‰¹å¾å›¾ä» (N, H, W, C) è½¬æ¢ä¸º (N, C, H, W)
        f0, f1, f2, f3 = [feat.permute(0, 3, 1, 2) for feat in features]
        # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…

        # 2. è§£ç è¿‡ç¨‹ (ç°åœ¨å¯ä»¥æ­£å¸¸å·¥ä½œäº†)
        b = self.bottleneck(f3)
        d1 = self.up1(b, f2)
        d2 = self.up2(d1, f1)
        d3 = self.up3(d2, f0)

        d4 = self.up4(d3)
        out = nn.functional.interpolate(d4, scale_factor=2, mode='bilinear', align_corners=True)

        return self.final_conv(out)


# ==============================================================================
# --- 4. ä¸»è®­ç»ƒæµç¨‹ (å·²ä¿®æ”¹ä¸ºæŒ‰æ­¥æ•°è¯„ä¼°) ---
# ==============================================================================

def evaluate_and_save(model, eval_loader, criterion, device, best_eval_loss, global_step):
    """ç‹¬ç«‹çš„è¯„ä¼°å‡½æ•°"""
    model.eval()
    eval_loss = 0.0
    # eval_pbar = tqdm(eval_loader, desc=f"Step {global_step} [éªŒè¯]", file=sys.stdout)

    with torch.no_grad():
        for batch in eval_loader:  # eval_pbar:
            inputs = batch["input_pixel_values"].to(device)
            targets = batch["output_pixel_values"].to(device)

            with torch.cuda.amp.autocast(enabled=FP16):
                outputs = model(inputs)
                loss = criterion(outputs, targets)

            eval_loss += loss.item()
            # eval_pbar.set_postfix({"loss": loss.item()})

    avg_eval_loss = eval_loss / len(eval_loader)

    log_message = f"Step {global_step} | Eval Loss: {avg_eval_loss:.6f}"
    print(log_message)
    with open(LOG_FILE, "a") as f:
        f.write(log_message + "\n")

    if avg_eval_loss < best_eval_loss:
        best_eval_loss = avg_eval_loss
        save_path = os.path.join(OUTPUT_DIR, "best_model.pth")
        torch.save(model.state_dict(), save_path)
        print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³ {save_path} (Step: {global_step}, Eval Loss: {best_eval_loss:.6f})")

    return best_eval_loss


def save_comparison_images(model, eval_loader, device, global_step):
    """ç‹¬ç«‹çš„ä¿å­˜å¯¹æ¯”å›¾å‡½æ•°"""
    model.eval()
    with torch.no_grad():
        sample_batch = next(iter(eval_loader))
        sample_input = sample_batch["input_pixel_values"][:20].to(device)
        sample_output = model(sample_input)

        to_pil = transforms.ToPILImage()
        for i in range(sample_input.size(0)):
            input_img = to_pil(sample_batch["input_pixel_values"][i])
            target_img = to_pil(sample_batch["output_pixel_values"][i])
            pred_img = to_pil(sample_output[i].cpu())

            comparison_img = Image.new('RGB', (IMAGE_SIZE[0] * 3, IMAGE_SIZE[1]))
            comparison_img.paste(input_img, (0, 0))
            comparison_img.paste(pred_img, (IMAGE_SIZE[0], 0))
            comparison_img.paste(target_img, (IMAGE_SIZE[0] * 2, 0))

            comparison_img.save(os.path.join(OUTPUT_DIR, f"step_{global_step}_sample_{i}.png"))
    print(f"å·²ä¿å­˜å¯¹æ¯”å›¾åƒè‡³ {OUTPUT_DIR}")


def main():
    # --- å‡†å¤‡ç¯å¢ƒ ---
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # --- å‡†å¤‡æ•°æ®é›† ---
    print("å‡†å¤‡æ•°æ®é›†...")
    full_dataset = ImageToImageDataset(root_dir=DATASET_DIR, image_size=IMAGE_SIZE)
    # full_dataset = ImageToImageDataset_maze(root_dir=DATASET_DIR, image_size=IMAGE_SIZE)
    train_size = int(0.998 * len(full_dataset))
    eval_size = len(full_dataset) - train_size
    train_dataset, eval_dataset = random_split(full_dataset, [train_size, eval_size])
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, prefetch_factor=1,
        pin_memory=True)
    eval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)
    print(f"æ•°æ®é›†åˆ’åˆ†: {len(train_dataset)} è®­ç»ƒæ ·æœ¬, {len(eval_dataset)} éªŒè¯æ ·æœ¬ã€‚")

    # --- åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ---
    print("åˆå§‹åŒ– Swin-Unet æ¨¡å‹...")

    # â˜…â˜…â˜… å…³é”®æ”¹åŠ¨ï¼šåˆ†ä¸¤æ­¥åˆ›å»ºå’ŒåŠ è½½æ¨¡å‹ â˜…â˜…â˜…

    # 1. åˆ›å»ºæ¨¡å‹ç»“æ„ï¼Œä½†è®¾ç½® pretrained=Falseï¼Œé¿å…åœ¨çº¿ä¸‹è½½
    print(f"åˆ›å»ºæ¨¡å‹ç»“æ„: {MODEL_NAME}")
    model = SwinUnet(model_name=MODEL_NAME, pretrained=False).to(device)

    # 2. ä»æœ¬åœ°æ–‡ä»¶åŠ è½½é¢„è®­ç»ƒæƒé‡
    if os.path.exists(PRETRAINED_MODEL_PATH):
        print(f"ä»æœ¬åœ°è·¯å¾„åŠ è½½é¢„è®­ç»ƒæƒé‡: {PRETRAINED_MODEL_PATH}")
        # åŠ è½½æ•´ä¸ªæ¨¡å‹çš„çŠ¶æ€å­—å…¸
        state_dict = torch.load(PRETRAINED_MODEL_PATH, map_location=device)

        # SwinUnetåŒ…å«ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œè€Œæˆ‘ä»¬çš„æƒé‡æ–‡ä»¶åªåŒ…å«ç¼–ç å™¨éƒ¨åˆ†ã€‚
        # æˆ‘ä»¬éœ€è¦æ™ºèƒ½åœ°åªåŠ è½½ç¼–ç å™¨çš„æƒé‡ã€‚

        # æå–åªå±äºç¼–ç å™¨çš„æƒé‡
        encoder_state_dict = {}
        for k, v in state_dict.items():
            # æˆ‘ä»¬çš„SwinUnetæ¨¡å‹ä¸­ï¼Œç¼–ç å™¨è¢«å‘½åä¸º self.encoder
            # timmä¸‹è½½çš„æƒé‡é”®åæ²¡æœ‰è¿™ä¸ªå‰ç¼€ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦åŠ ä¸Š
            encoder_key = f"encoder.{k}"
            if encoder_key in model.state_dict():
                encoder_state_dict[encoder_key] = v

        # åŠ è½½æƒé‡ï¼Œstrict=Falseå…è®¸åªåŠ è½½éƒ¨åˆ†æƒé‡ï¼ˆåªåŠ è½½ç¼–ç å™¨ï¼Œä¸åŠ è½½è§£ç å™¨ï¼‰
        model.load_state_dict(encoder_state_dict, strict=False)
        print("ç¼–ç å™¨æƒé‡åŠ è½½æˆåŠŸï¼è§£ç å™¨æƒé‡å°†éšæœºåˆå§‹åŒ–ã€‚")

    else:
        print(f"è­¦å‘Š: æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶äº '{PRETRAINED_MODEL_PATH}'ã€‚å°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")

    criterion = nn.MSELoss()
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    scaler = torch.cuda.amp.GradScaler(enabled=FP16)

    # --- å¼€å§‹è®­ç»ƒ ---
    print(f"ğŸš€ å¼€å§‹ Image-to-Image è®­ç»ƒï¼Œæ¯ {EVAL_EVERY_N_STEPS} æ­¥è¯„ä¼°ä¸€æ¬¡...")
    best_eval_loss = float('inf')
    global_step = 0

    for epoch in range(NUM_EPOCHS):
        model.train()
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{NUM_EPOCHS}")

        for batch in train_pbar:
            inputs = batch["input_pixel_values"].to(device)
            targets = batch["output_pixel_values"].to(device)

            optimizer.zero_grad()

            with torch.cuda.amp.autocast(enabled=FP16):
                outputs = model(inputs)
                loss = criterion(outputs, targets)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            global_step += 1
            train_pbar.set_postfix({"train_loss": f"{loss.item():.6f}", "step": global_step})

            # --- æŒ‰æ­¥æ•°è¿›è¡Œè¯„ä¼°å’Œä¿å­˜ ---
            if global_step % EVAL_EVERY_N_STEPS==0:
                # è°ƒç”¨è¯„ä¼°å‡½æ•°
                best_eval_loss = evaluate_and_save(
                    model, eval_loader, criterion, device, best_eval_loss, global_step
                )
                # è¯„ä¼°ååˆ‡å›è®­ç»ƒæ¨¡å¼
                model.train()

            if global_step % SAVE_IMAGE_EVERY_N_STEPS==0:
                # è°ƒç”¨ä¿å­˜å›¾åƒå‡½æ•°
                save_comparison_images(model, eval_loader, device, global_step)
                # è¯„ä¼°ååˆ‡å›è®­ç»ƒæ¨¡å¼
                model.train()

    print("âœ… è®­ç»ƒå®Œæˆï¼")


if __name__=="__main__":
    main()
