import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer, TrainerCallback
)
from peft import get_peft_model, LoraConfig, TaskType
from PIL import Image
from torchvision import transforms
import pandas as pd
from tqdm import tqdm
import time
from transformers import TrainerState, TrainerControl


# ==============================================================================
# --- 1. é…ç½®åŒºåŸŸ ---
# ==============================================================================

# --- æ¨¡å‹é…ç½® ---
# æ–‡æœ¬ç¼–ç å™¨ï¼Œä½¿ç”¨ä½ çš„ç›®æ ‡æ¨¡å‹
TEXT_MODEL_NAME = "qwen2_0.5b"
# LoRAé…ç½®
LORA_R = 16
LORA_ALPHA = 32
# ä½ çš„U-Netè§£ç å™¨éœ€è¦çš„è¾“å…¥é€šé“æ•°ï¼Œç­‰äºQwen2çš„éšè—å±‚ç»´åº¦
# Qwen2-0.5B çš„ hidden_size æ˜¯ 896
DECODER_INPUT_CHANNELS = 896

# --- å›¾åƒé…ç½® ---
# IMAGE_SIZE = (224, 224)
# IMAGE_SIZE = (256, 256)
IMAGE_SIZE = (240, 240)
OUTPUT_CHANNELS = 3  # RGB

# --- æ•°æ®é›†é…ç½® ---
# ä½ çš„æ•°æ®é›†æ ¹ç›®å½•ï¼Œé‡Œé¢åº”è¯¥åŒ…å«ä¸€ä¸ª"images"æ–‡ä»¶å¤¹å’Œä¸€ä¸ª"captions.csv"æ–‡ä»¶
# DATASET_DIR = "./checkerboard_dataset"
# DATASET_DIR = "./triangle_dataset"
# DATASET_DIR = "./cube_dataset_final_highlight"
DATASET_DIR = "./ca_render_dataset_240"
CAPTIONS_FILE = "metadata.csv"  # csvæ–‡ä»¶åº”åŒ…å« 'image_file' å’Œ 'caption' ä¸¤åˆ—

# --- è®­ç»ƒå‚æ•° ---
OUTPUT_DIR = "./autodl-tmp/checkpoints_qwen2_text2image_ca"
BATCH_SIZE = 32  # å›¾åƒç”Ÿæˆä»»åŠ¡é€šå¸¸éœ€è¦æ›´å°çš„batch size
LEARNING_RATE = 2e-4
NUM_EPOCHS = 1800
FP16 = torch.cuda.is_available()
EVAL_EVERY_N_STEPS = 100
LOGGING_STEPS = 20


# ==============================================================================
# --- 2. æ ¸å¿ƒæ¨¡å‹å®šä¹‰ (TextToImageModel) ---
# ==============================================================================
class SaveImagePredictionCallback(TrainerCallback):
    """ä¸€ä¸ªåœ¨è¯„ä¼°æ—¶ä¿å­˜æ¨¡å‹é¢„æµ‹å›¾åƒå¯¹æ¯”çš„å›è°ƒå‡½æ•°ã€‚"""

    def __init__(self, eval_dataset, output_dir, num_samples=32):
        super().__init__()
        self.output_dir = os.path.join(output_dir, "eval_predictions")
        os.makedirs(self.output_dir, exist_ok=True)

        # ä»è¯„ä¼°æ•°æ®é›†ä¸­å›ºå®šå–å‡ ä¸ªæ ·æœ¬ï¼Œç¡®ä¿æ¯æ¬¡è¯„ä¼°éƒ½ç”¨åŒæ ·çš„æ ·æœ¬è¿›è¡Œå¯¹æ¯”
        self.num_samples = min(num_samples, len(eval_dataset))
        self.samples = [eval_dataset[i] for i in range(self.num_samples)]

        print(f"âœ… [Callback] å·²åˆå§‹åŒ–ï¼Œå°†åœ¨æ¯æ¬¡è¯„ä¼°æ—¶ä¿å­˜ {self.num_samples} å¼ å¯¹æ¯”å›¾è‡³ {self.output_dir}")

    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
        """åœ¨è¯„ä¼°äº‹ä»¶ç»“æŸæ—¶è¢«è°ƒç”¨ã€‚"""
        # ä»kwargsä¸­è·å–æ¨¡å‹å’Œåˆ†è¯å™¨
        model = kwargs["model"]
        tokenizer = model.tokenizer  # ä»å¤åˆæ¨¡å‹ä¸­è·å–tokenizer
        try:
            device = next(model.parameters()).device
        except StopIteration:
            # å¦‚æœæ¨¡å‹æ²¡æœ‰ä»»ä½•å‚æ•°ï¼ˆè™½ç„¶ä¸å¤ªå¯èƒ½ï¼‰ï¼Œå¯ä»¥ä»TrainingArgumentsè·å–
            device = args.device

        # å‡†å¤‡è¾“å…¥æ•°æ®
        input_ids = torch.stack([s['input_ids'] for s in self.samples]).to(device)
        attention_mask = torch.stack([s['attention_mask'] for s in self.samples]).to(device)

        # å°†æ¨¡å‹ç½®äºè¯„ä¼°æ¨¡å¼ï¼Œå¹¶å…³é—­æ¢¯åº¦è®¡ç®—
        model.eval()
        with torch.no_grad():
            # ä½¿ç”¨å½“å‰æ¨¡å‹ç”Ÿæˆå›¾åƒ
            generated_images = model(input_ids=input_ids, attention_mask=attention_mask)

        # å°†ç”Ÿæˆçš„å›¾åƒå’Œç›®æ ‡å›¾åƒè½¬æ¢å›PIL Imageæ ¼å¼
        to_pil = transforms.ToPILImage()
        gen_pils = [to_pil(img.cpu()) for img in generated_images]
        target_pils = [to_pil(s['labels']) for s in self.samples]  # Datasetè¿”å›'labels'

        # å°†è¾“å…¥æ–‡æœ¬ä¹Ÿè§£ç å‡ºæ¥ï¼Œä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
        captions = [
            tokenizer.decode(ids, skip_special_tokens=True)
            for ids in input_ids
        ]

        # æ‹¼æ¥å¹¶ä¿å­˜å›¾åƒ
        for i in range(self.num_samples):
            target_img = target_pils[i]
            gen_img = gen_pils[i]

            # åˆ›å»ºä¸€ä¸ªå®½åº¦ä¸ºä¸¤å€çš„å¤§å›¾
            comparison_img = Image.new('RGB', (target_img.width * 2, target_img.height))
            comparison_img.paste(target_img, (0, 0))
            comparison_img.paste(gen_img, (target_img.width, 0))

            # ç”¨global_stepå’Œcaptionæ¥å‘½åæ–‡ä»¶
            step = state.global_step
            # æ¸…ç†captionï¼Œä½¿å…¶é€‚åˆåšæ–‡ä»¶å
            caption_slug = captions[i][:30].replace(" ", "_").replace("/", "")
            save_path = os.path.join(self.output_dir, f"step_{step}_sample_{i}_{caption_slug}.png")
            comparison_img.save(save_path)

        print(f"ğŸ’¾ [Callback] å·²åœ¨æ­¥éª¤ {state.global_step} ä¿å­˜å¯¹æ¯”å›¾åƒã€‚")

        # è®°å¾—å°†æ¨¡å‹åˆ‡å›è®­ç»ƒæ¨¡å¼ï¼Œä»¥ä¾¿è®­ç»ƒç»§ç»­
        model.train()


class UpsampleBlock(nn.Module):
    """ä½ çš„U-Netä¸Šé‡‡æ ·å—ï¼Œä¿æŒä¸å˜"""

    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.upsample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)
        self.conv = nn.Sequential(
            nn.Conv2d(out_channels * 2, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True)
        )

    def forward(self, x, skip_connection):
        x = self.upsample(x)
        if x.shape[2:]!=skip_connection.shape[2:]:
            x = nn.functional.interpolate(x, size=skip_connection.shape[2:], mode='bilinear', align_corners=False)
        x = torch.cat([skip_connection, x], dim=1)
        return self.conv(x)


class ImageDecoder(nn.Module):
    """å°†ä½ çš„U-Netè§£ç å™¨éƒ¨åˆ†å°è£…æˆä¸€ä¸ªç‹¬ç«‹çš„æ¨¡å—"""

    def __init__(self, text_feature_dim, image_size=224, output_channels=3):
        super().__init__()
        self.image_size = image_size

        # æ¨¡ä»¿Swin-Unetçš„ç»“æ„ï¼Œå®šä¹‰è§£ç å™¨éœ€è¦çš„ç‰¹å¾å›¾å°ºå¯¸å’Œé€šé“æ•°
        decoder_channels = [128, 256, 512, 1024]
        feature_map_sizes = [
            (image_size // 4, image_size // 4),  # f0
            (image_size // 8, image_size // 8),  # f1
            (image_size // 16, image_size // 16),  # f2
            (image_size // 32, image_size // 32)  # f3
        ]

        # æ ¸å¿ƒï¼šå°†æ–‡æœ¬å‘é‡æŠ•å½±æˆä¸åŒå°ºå¯¸çš„2Dç‰¹å¾å›¾ï¼Œä½œä¸º"å‡çš„"è·³è·ƒè¿æ¥
        self.skip_projections = nn.ModuleList([
            self._create_projection(text_feature_dim, ch, size)
            for ch, size in zip(decoder_channels, feature_map_sizes)
        ])

        # U-Netè§£ç å™¨ç»“æ„
        self.bottleneck_conv = nn.Sequential(
            nn.Conv2d(decoder_channels[3], decoder_channels[3], kernel_size=3, padding=1),
            nn.ReLU(inplace=True)
        )
        self.up1 = UpsampleBlock(decoder_channels[3], decoder_channels[2])
        self.up2 = UpsampleBlock(decoder_channels[2], decoder_channels[1])
        self.up3 = UpsampleBlock(decoder_channels[1], decoder_channels[0])

        # æœ€ç»ˆè¾“å‡ºå±‚
        self.final_up = nn.Sequential(
            nn.ConvTranspose2d(decoder_channels[0], decoder_channels[0] // 2, kernel_size=2, stride=2),
            nn.ReLU(inplace=True)
        )
        self.final_conv = nn.Sequential(
            nn.Conv2d(decoder_channels[0] // 2, output_channels, kernel_size=1),
            nn.Sigmoid()  # æˆ– nn.Tanh()
        )

        self.feature_map_sizes = feature_map_sizes
        self.decoder_channels = decoder_channels

    def _create_projection(self, text_dim, out_channels, size):
        return nn.Sequential(
            nn.Linear(text_dim, text_dim // 2), nn.ReLU(),
            nn.Linear(text_dim // 2, out_channels * size[0] * size[1])
        )

    def forward(self, text_feature):
        # åˆ¶é€ _æ‰€æœ‰_è·³è·ƒè¿æ¥
        fakes = [
            proj(text_feature).view(
                text_feature.size(0), ch, size[0], size[1]
            ) for proj, ch, size in zip(self.skip_projections, self.decoder_channels, self.feature_map_sizes)
        ]
        f0_fake, f1_fake, f2_fake, f3_fake = fakes

        # è¿è¡Œè§£ç å™¨
        b = self.bottleneck_conv(f3_fake)
        d1 = self.up1(b, f2_fake)
        d2 = self.up2(d1, f1_fake)
        d3 = self.up3(d2, f0_fake)

        # ç”Ÿæˆæœ€ç»ˆå›¾åƒ
        d4 = self.final_up(d3)
        out = nn.functional.interpolate(d4, size=(self.image_size, self.image_size), mode='bilinear',
            align_corners=False)
        return self.final_conv(out)


class TextToImageModel(nn.Module):
    """å¤åˆæ¨¡å‹ï¼šå°è£…Qwen2 (PEFT) å’Œ å›¾åƒè§£ç å™¨"""

    def __init__(self, text_model_name, image_size, output_channels):
        super().__init__()
        # åŠ è½½åŸºç¡€çš„Qwen2æ¨¡å‹å’ŒTokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(text_model_name, trust_remote_code=True)
        base_model = AutoModelForCausalLM.from_pretrained(text_model_name, trust_remote_code=True)

        # åº”ç”¨LoRA
        lora_config = LoraConfig(
            r=LORA_R, lora_alpha=LORA_ALPHA,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],  # Qwen2çš„å¸¸ç”¨ç›®æ ‡æ¨¡å—
            lora_dropout=0.1, bias="none", task_type=TaskType.CAUSAL_LM,
        )
        self.text_encoder = get_peft_model(base_model, lora_config)
        print("Qwen2 LoRAæ¨¡å‹å·²åˆ›å»ºã€‚")
        self.text_encoder.print_trainable_parameters()

        # åˆå§‹åŒ–å›¾åƒè§£ç å™¨
        text_feature_dim = self.text_encoder.config.hidden_size
        self.decoder = ImageDecoder(text_feature_dim, image_size, output_channels)

    def forward(self, input_ids, attention_mask, **kwargs):
        # 1. ä»Qwen2è·å–æ–‡æœ¬ç‰¹å¾
        # æˆ‘ä»¬éœ€è¦çš„æ˜¯æœ€åä¸€ä¸ªéšè—å±‚çŠ¶æ€ï¼Œè€Œä¸æ˜¯lm_headçš„è¾“å‡º
        # .model ä¼šç»•è¿‡lm_headï¼Œç›´æ¥è°ƒç”¨åº•å±‚çš„transformeræ¨¡å—
        encoder_outputs = self.text_encoder.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )
        # encoder_outputs æ˜¯ä¸€ä¸ª BaseModelOutputWithPast å¯¹è±¡ï¼Œ
        # å®ƒåŒ…å« .last_hidden_state

        # è·å–æ‰€æœ‰tokençš„éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º [batch_size, sequence_length, hidden_size]
        hidden_states = encoder_outputs.hidden_states

        # è·å–æœ€åä¸€å±‚çš„éšè—çŠ¶æ€ï¼Œå…¶å½¢çŠ¶ä¸º [batch_size, sequence_length, hidden_size]
        last_hidden_state = hidden_states[-1]

        # 2. æå–ä»£è¡¨æ•´ä¸ªå¥å­è¯­ä¹‰çš„å‘é‡
        # (è¿™éƒ¨åˆ†é€»è¾‘ä¿æŒä¸å˜)
        sequence_lengths = torch.eq(attention_mask, 1).sum(-1) - 1

        # text_feature çš„å½¢çŠ¶æ˜¯ [batch_size, hidden_size]
        # ä¾‹å¦‚ï¼š[32, 896]
        text_feature = last_hidden_state[
            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),
            sequence_lengths
        ]

        # 3. ç”¨æ­£ç¡®çš„æ–‡æœ¬ç‰¹å¾é©±åŠ¨å›¾åƒè§£ç å™¨
        generated_image = self.decoder(text_feature)
        return generated_image


# ==============================================================================
# --- 3. æ•°æ®é›†ä¸è®­ç»ƒå™¨å®šä¹‰ ---
# ==============================================================================

class TextToImageDataset(Dataset):
    def __init__(self, root_dir, captions_file, tokenizer, image_size):
        self.image_dir = os.path.join(root_dir, "images") # root_dir
        self.captions_df = pd.read_csv(os.path.join(root_dir, captions_file))
        self.tokenizer = tokenizer
        self.transform = transforms.Compose([
            transforms.Resize(image_size),
            transforms.ToTensor(),
        ])

    def __len__(self):
        return len(self.captions_df)

    def __getitem__(self, idx):
        row = self.captions_df.iloc[idx]
        caption = row['label']
        img_name = row['filename']

        # åŠ è½½å’Œè½¬æ¢ç›®æ ‡å›¾ç‰‡
        img_path = os.path.join(self.image_dir, img_name)
        try:
            image = Image.open(img_path).convert("RGB")
            target_pixels = self.transform(image)
        except FileNotFoundError:
            print(f"Warning: Image not found {img_path}, skipping.")
            # è¿”å›ä¸€ä¸ªdummyæ•°æ®ï¼Œcollate_fnä¼šå¤„ç†
            return None

        # åˆ†è¯æ–‡æœ¬
        # tokenized_caption = self.tokenizer(
        #     caption, padding="max_length", truncation=True,
        #     max_length=128, return_tensors="pt"
        # )
        input_ids = []
        for char in caption:
            input_ids.append(self.tokenizer.convert_tokens_to_ids(str(char)))
        if self.tokenizer.eos_token_id is not None:
            input_ids.append(151643)  # tokenizer.eos_token_id)
        else:
            print("è­¦å‘Š: Tokenizeræ²¡æœ‰å®šä¹‰eos_token_idã€‚")
        attention_mask = [1] * len(input_ids)
        attention_mask[-1] = 0
        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
            "labels": target_pixels  # ä½¿ç”¨ 'pixel_values' ä½œä¸ºkey
        }
        # return {
        #     "input_ids": tokenized_caption['input_ids'].squeeze(0),
        #     "attention_mask": tokenized_caption['attention_mask'].squeeze(0),
        #     "pixel_values": target_pixels # ä½¿ç”¨ 'pixel_values' ä½œä¸ºkey
        # }


class ImageGenTrainer(Trainer):
    """è‡ªå®šä¹‰Trainerï¼Œç”¨äºè®¡ç®—å›¾åƒé‡å»ºæŸå¤±"""

    def compute_loss(self, model, inputs, return_outputs=False):
        # ä»è¾“å…¥ä¸­åˆ†ç¦»å‡ºç›®æ ‡å›¾åƒ
        # print(list(inputs.keys()))
        labels = inputs.pop("labels")

        # è·å–æ¨¡å‹ç”Ÿæˆçš„å›¾åƒ
        # **inputs åŒ…å« 'input_ids' å’Œ 'attention_mask'
        generated_images = model(**inputs)

        # ä½¿ç”¨MSEæˆ–L1ä½œä¸ºå›¾åƒé‡å»ºæŸå¤±
        loss_fct = nn.MSELoss()
        loss = loss_fct(generated_images, labels)

        return (loss, {"outputs": generated_images}) if return_outputs else loss

    def evalua1te(
            self,
            eval_dataset=None,
            ignore_keys=None,
            metric_key_prefix='eval',
    ):
        """
        é‡å†™è¯„ä¼°æ–¹æ³•ï¼Œæ‰‹åŠ¨è®¡ç®—å¹¶æ³¨å…¥eval_lossã€‚
        """
        # 1. è·å–è¯„ä¼°æ•°æ®åŠ è½½å™¨
        eval_dataloader = self.get_eval_dataloader(eval_dataset)
        start_time = time.time()

        # 2. è°ƒç”¨çˆ¶ç±»çš„ evaluation_loop æ¥è·å–åŸºç¡€æŒ‡æ ‡å’Œæ¨¡å‹è¾“å‡º
        #    æ³¨æ„ï¼šè¿™ä¸ªloopæœ¬èº«ä¸ä¼šè®¡ç®—æˆ‘ä»¬çš„è‡ªå®šä¹‰loss
        output = self.evaluation_loop(
            eval_dataloader,
            description="Evaluation",
            prediction_loss_only=True,  # è®¾ä¸ºTrueï¼Œå› ä¸ºå®ƒä¸çŸ¥é“æ€ä¹ˆç®—lossï¼Œå¯ä»¥è·³è¿‡
            metric_key_prefix=metric_key_prefix,
        )

        # 3. æ‰‹åŠ¨è®¡ç®—æˆ‘ä»¬è‡ªå·±çš„è¯„ä¼°æŸå¤±
        total_eval_loss = 0.0
        num_eval_samples = 0

        # å°†æ¨¡å‹ç½®äºè¯„ä¼°æ¨¡å¼
        model = self._wrap_model(self.model, training=False)
        model.eval()

        for step, inputs in enumerate(eval_dataloader):
            inputs = self._prepare_inputs(inputs)
            with torch.no_grad():
                # ç›´æ¥è°ƒç”¨compute_lossæ¥å¾—åˆ°æŸå¤±å€¼
                loss, _ = self.compute_loss(model, inputs, return_outputs=True)

            # ç´¯åŠ æŸå¤± (æ³¨æ„å¤šGPUæƒ…å†µä¸‹çš„èšåˆ)
            total_eval_loss += loss.item() * len(inputs["input_ids"])
            num_eval_samples += len(inputs["input_ids"])

        # 4. è®¡ç®—å¹³å‡æŸå¤±
        avg_eval_loss = total_eval_loss / num_eval_samples

        # 5. å°†æˆ‘ä»¬çš„eval_lossæ·»åŠ åˆ°æŒ‡æ ‡å­—å…¸ä¸­
        #    output.metrics æ˜¯çˆ¶ç±»æ–¹æ³•è¿”å›çš„åŸºç¡€æŒ‡æ ‡ï¼ˆå¦‚runtimeï¼‰
        #    æˆ‘ä»¬å¿…é¡»ä½¿ç”¨ 'eval_loss' è¿™ä¸ªé”®åï¼
        output.metrics[f"{metric_key_prefix}_loss"] = avg_eval_loss
        print('eval loss: ', avg_eval_loss)

        end_time = time.time()
        # è¡¥å……å…¶ä»–ç¼ºå¤±çš„æŒ‡æ ‡
        output.metrics[f"{metric_key_prefix}_runtime"] = end_time - start_time
        output.metrics[f"{metric_key_prefix}_samples_per_second"] = num_eval_samples / (end_time - start_time)

        # 6. è®°å½•æ—¥å¿—å¹¶è¿”å›
        # self.log(output.metrics) # è¿™è¡Œç”±traineråœ¨å¤–éƒ¨è°ƒç”¨ï¼Œæˆ‘ä»¬ä¸ç”¨è‡ªå·±è°ƒ

        return output.metrics


class SaveDecoderCallback(TrainerCallback):
    """ä¸€ä¸ªå›è°ƒï¼Œç”¨äºå®šæœŸä¿å­˜è§£ç å™¨éƒ¨åˆ†çš„æƒé‡"""

    def __init__(self, save_dir):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

    def on_save(self, args, state, control, **kwargs):
        # on_save åœ¨æ¯æ¬¡ checkpoint ä¿å­˜æ—¶è§¦å‘
        model = kwargs["model"]
        save_path = os.path.join(self.save_dir, f"decoder_step_{state.global_step}.pth")
        if state.global_step % 1000000==0:
            torch.save(model.decoder.state_dict(), save_path)
            print(f"ğŸ’¾ å•ç‹¬ä¿å­˜å›¾åƒè§£ç å™¨æƒé‡ -> {save_path}")


# ==============================================================================
# --- 4. ä¸»æ‰§è¡Œæµç¨‹ ---
# ==============================================================================

def main():
    # --- 1. åˆå§‹åŒ–å¤åˆæ¨¡å‹ ---
    print("ğŸš€ åˆå§‹åŒ–Text-to-Imageæ¨¡å‹...")
    model = TextToImageModel(
        text_model_name=TEXT_MODEL_NAME,
        image_size=IMAGE_SIZE[0],
        output_channels=OUTPUT_CHANNELS
    )

    # --- 2. å…³é”®ï¼šè®¾ç½®å¯è®­ç»ƒå‚æ•° ---
    # é»˜è®¤ LoRA å·²ç»è®¾ç½®å¥½äº† text_encoder çš„å¯è®­ç»ƒå‚æ•°
    # æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è§£å†»ä¸‹æ¸¸çš„è§£ç å™¨
    for param in model.decoder.parameters():
        param.requires_grad = True

    print("\nâœ… å¯è®­ç»ƒå‚æ•°è®¾ç½®å®Œæˆ:")
    model.text_encoder.print_trainable_parameters()

    # ç»Ÿè®¡è§£ç å™¨çš„å¯è®­ç»ƒå‚æ•°
    decoder_params = sum(p.numel() for p in model.decoder.parameters() if p.requires_grad)
    print(f"å›¾åƒè§£ç å™¨ (ImageDecoder) å¯è®­ç»ƒå‚æ•°: {decoder_params / 1e6:.2f}M")

    # --- 3. å‡†å¤‡æ•°æ®é›† ---
    print("\nğŸ“¦ å‡†å¤‡æ•°æ®é›†ä¸­...")
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    if not os.path.exists(DATASET_DIR) or not os.path.exists(os.path.join(DATASET_DIR, CAPTIONS_FILE)):
        print(f"âŒ é”™è¯¯: æ•°æ®é›†ç›®å½• '{DATASET_DIR}' æˆ– captions æ–‡ä»¶ '{CAPTIONS_FILE}' ä¸å­˜åœ¨ã€‚")
        print("è¯·åˆ›å»ºä¸€ä¸ªç›®å½•ï¼ŒåŒ…å« 'images' å­ç›®å½•å’Œ 'captions.csv' æ–‡ä»¶ã€‚")
        # åˆ›å»ºä¸€ä¸ªç¤ºä¾‹ä»¥ä¾›å‚è€ƒ
        os.makedirs(os.path.join(DATASET_DIR, 'images'), exist_ok=True)
        sample_df = pd.DataFrame([
            {'image_file': 'sample1.png', 'caption': 'a red circle on a white background'},
            {'image_file': 'sample2.png', 'caption': 'a blue square fading to green'}
        ])
        sample_df.to_csv(os.path.join(DATASET_DIR, CAPTIONS_FILE), index=False)
        print("å·²åˆ›å»ºç¤ºä¾‹ captions.csvã€‚è¯·æ”¾å…¥ä½ çš„æ•°æ®ã€‚")
        return

    full_dataset = TextToImageDataset(
        root_dir=DATASET_DIR,
        captions_file=CAPTIONS_FILE,
        tokenizer=model.tokenizer,
        image_size=IMAGE_SIZE
    )

    # è¿‡æ»¤æ‰åŠ è½½å¤±è´¥çš„Noneé¡¹
    # full_dataset.samples = [s for s in full_dataset.samples if s is not None]

    if len(full_dataset)==0:
        print("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ä½ çš„æ•°æ®é›†è·¯å¾„å’Œå†…å®¹ã€‚")
        return

    train_size = int(0.995 * len(full_dataset))
    eval_size = len(full_dataset) - train_size
    train_dataset, eval_dataset = random_split(full_dataset, [train_size, eval_size])
    print(f"æ•°æ®é›†åˆ’åˆ†: {len(train_dataset)} è®­ç»ƒ, {len(eval_dataset)} éªŒè¯")

    save_image_callback = SaveImagePredictionCallback(
        eval_dataset=eval_dataset,
        output_dir=OUTPUT_DIR,
        num_samples=32  # ä½ æƒ³ä¿å­˜çš„æ ·æœ¬æ•°é‡
    )

    # --- 4. è®¾ç½®è®­ç»ƒå‚æ•°å’Œè®­ç»ƒå™¨ ---
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        learning_rate=LEARNING_RATE,
        num_train_epochs=NUM_EPOCHS,
        evaluation_strategy="steps",
        eval_steps=EVAL_EVERY_N_STEPS,
        save_strategy="steps",
        save_steps=EVAL_EVERY_N_STEPS*10000,
        fp16=FP16,
        logging_dir=f"{OUTPUT_DIR}/logs",
        logging_steps=LOGGING_STEPS,
        # save_total_limit=3,
        # report_to="none", # æˆ–è€… "tensorboard"
        remove_unused_columns=False,
        dataloader_num_workers=2,  # <-- è®¾ç½®ä¸ºä½ CPUæ ¸å¿ƒæ•°çš„ä¸€åŠæˆ–æ›´å¤šï¼Œæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
        dataloader_prefetch_factor=1,
        dataloader_pin_memory=True,  # <-- å¼ºçƒˆå»ºè®®å¼€å¯
        save_safetensors=False,
    )

    trainer = ImageGenTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        callbacks=[save_image_callback, SaveDecoderCallback(save_dir=OUTPUT_DIR)]  # æ·»åŠ è‡ªå®šä¹‰å›è°ƒ
    )

    # --- 5. å¼€å§‹è®­ç»ƒ ---
    print("\nğŸ”¥ å¼€å§‹è®­ç»ƒï¼")
    trainer.train()

    # --- 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    print("\nâœ… è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    # ä¿å­˜LoRAé€‚é…å™¨
    model.text_encoder.save_pretrained(os.path.join(OUTPUT_DIR, "final_lora_adapter"))
    # ä¿å­˜è§£ç å™¨
    torch.save(model.decoder.state_dict(), os.path.join(OUTPUT_DIR, "final_decoder.pth"))
    # ä¿å­˜tokenizer
    model.tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, "final_tokenizer"))
    print(f"æ¨¡å‹å·²ä¿å­˜è‡³ {OUTPUT_DIR}")


if __name__=="__main__":
    main()